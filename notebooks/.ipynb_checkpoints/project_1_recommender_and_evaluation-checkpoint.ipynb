{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "alike-morgan",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from collections import defaultdict\n",
    "\n",
    "# Fix the dying kernel problem (only a problem in some installations - you can remove it, if it works without it)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-knitting",
   "metadata": {},
   "source": [
    "# Load the dataset for recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "victorian-bottom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>term</th>\n",
       "      <th>length_of_stay_bucket</th>\n",
       "      <th>rate_plan</th>\n",
       "      <th>room_segment</th>\n",
       "      <th>n_people_bucket</th>\n",
       "      <th>weekend_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>WinterVacation</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[260-360]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>WinterVacation</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>WinterVacation</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>WinterVacation</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>WinterVacation</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[0-160]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>Easter</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[260-360]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[260-360]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[1-1]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[0-160]</td>\n",
       "      <td>[1-1]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[1-1]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[1-1]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>HighSeason</td>\n",
       "      <td>[8-inf]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>LowSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id            term length_of_stay_bucket rate_plan  \\\n",
       "0         1        0  WinterVacation                 [2-3]  Standard   \n",
       "1         2        1  WinterVacation                 [2-3]  Standard   \n",
       "2         3        2  WinterVacation                 [2-3]  Standard   \n",
       "3         4        3  WinterVacation                 [4-7]  Standard   \n",
       "4         5        4  WinterVacation                 [4-7]  Standard   \n",
       "5         6        5          Easter                 [4-7]  Standard   \n",
       "6         7        6       OffSeason                 [2-3]  Standard   \n",
       "7         8        7      HighSeason                 [2-3]  Standard   \n",
       "8         9        8      HighSeason                 [2-3]  Standard   \n",
       "9         8        7      HighSeason                 [2-3]  Standard   \n",
       "10        8        7      HighSeason                 [2-3]  Standard   \n",
       "11       10        9      HighSeason                 [2-3]  Standard   \n",
       "12       11        9      HighSeason                 [2-3]  Standard   \n",
       "13       12       10      HighSeason               [8-inf]  Standard   \n",
       "14       15       11       LowSeason                 [2-3]  Standard   \n",
       "\n",
       "   room_segment n_people_bucket weekend_stay  \n",
       "0     [260-360]         [5-inf]        False  \n",
       "1     [160-260]           [3-4]         True  \n",
       "2     [160-260]           [2-2]        False  \n",
       "3     [160-260]           [3-4]         True  \n",
       "4       [0-160]           [2-2]         True  \n",
       "5     [260-360]         [5-inf]         True  \n",
       "6     [260-360]         [5-inf]         True  \n",
       "7     [160-260]           [1-1]         True  \n",
       "8       [0-160]           [1-1]         True  \n",
       "9     [160-260]           [1-1]         True  \n",
       "10    [160-260]           [1-1]         True  \n",
       "11    [160-260]           [3-4]         True  \n",
       "12    [160-260]           [3-4]         True  \n",
       "13    [160-260]           [3-4]         True  \n",
       "14    [160-260]         [5-inf]        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = os.path.join(\"../data\", \"hotel_data\")\n",
    "\n",
    "interactions_df = pd.read_csv(os.path.join(data_path, \"hotel_data_interactions_df.csv\"), index_col=0)\n",
    "\n",
    "base_item_features = ['term', 'length_of_stay_bucket', 'rate_plan', 'room_segment', 'n_people_bucket', 'weekend_stay']\n",
    "\n",
    "column_values_dict = {\n",
    "    'term': ['WinterVacation', 'Easter', 'OffSeason', 'HighSeason', 'LowSeason', 'MayLongWeekend', 'NewYear', 'Christmas'],\n",
    "    'length_of_stay_bucket': ['[0-1]', '[2-3]', '[4-7]', '[8-inf]'],\n",
    "    'rate_plan': ['Standard', 'Nonref'],\n",
    "    'room_segment': ['[0-160]', '[160-260]', '[260-360]', '[360-500]', '[500-900]'],\n",
    "    'n_people_bucket': ['[1-1]', '[2-2]', '[3-4]', '[5-inf]'],\n",
    "    'weekend_stay': ['True', 'False']\n",
    "}\n",
    "\n",
    "interactions_df['term'] = pd.Categorical(\n",
    "    interactions_df['term'], categories=column_values_dict['term'])\n",
    "interactions_df['length_of_stay_bucket'] = pd.Categorical(\n",
    "    interactions_df['length_of_stay_bucket'], categories=column_values_dict['length_of_stay_bucket'])\n",
    "interactions_df['rate_plan'] = pd.Categorical(\n",
    "    interactions_df['rate_plan'], categories=column_values_dict['rate_plan'])\n",
    "interactions_df['room_segment'] = pd.Categorical(\n",
    "    interactions_df['room_segment'], categories=column_values_dict['room_segment'])\n",
    "interactions_df['n_people_bucket'] = pd.Categorical(\n",
    "    interactions_df['n_people_bucket'], categories=column_values_dict['n_people_bucket'])\n",
    "interactions_df['weekend_stay'] = interactions_df['weekend_stay'].astype('str')\n",
    "interactions_df['weekend_stay'] = pd.Categorical(\n",
    "    interactions_df['weekend_stay'], categories=column_values_dict['weekend_stay'])\n",
    "\n",
    "display(interactions_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "seven-policy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id                  0\n",
       "item_id                  0\n",
       "term                     0\n",
       "length_of_stay_bucket    0\n",
       "rate_plan                0\n",
       "room_segment             0\n",
       "n_people_bucket          6\n",
       "weekend_stay             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-procurement",
   "metadata": {},
   "source": [
    "It turns out that there were some rows, where `n_people = 0`, as a consequence `n_people_bucket` has been mapped to NaN. Let's fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "victorian-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = interactions_df.fillna('[1-1]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-third",
   "metadata": {},
   "source": [
    "# Define user features based on reservations\n",
    "\n",
    "The content-based recommenders will be forecasting the probability of interaction between user and item based on user features vector and item features vector:\n",
    "\n",
    "<center>\n",
    "$$\n",
    "    r_{u, i} = f(user\\_features, item\\_features)\n",
    "$$\n",
    "</center>\n",
    "\n",
    "<span style=\"color:red\"><font size=\"4\">**Task:**</font></span><br> \n",
    "Design numerical user features based on user reservations. Code the following method which for a given interactions DataFrame (it will be used in the fit method of the recommender) returns a DataFrame with user_id and user features as well as a list with names of user features (this will be important to select the right columns for an ML algorithm). Remember to name the columns differently than item features which you will create in the next task. Validate your features on users with several interactions (sample user ids are already given below).\n",
    "\n",
    "Ideas for user features:\n",
    "- Find the vector of most popular feature values from all user reservations and encode every feature with one-hot encoding.\n",
    "- For every reservation feature calculate the probability distribution of its values among all user's reservations.\n",
    "- For numerical buckets (length_of_stay, room_segment, n_people) you can calculate the average value for every user from their reservations (you will have to map the buckets back to numerical values before averaging them).\n",
    "\n",
    "Remember that you will have to select the best features (with the highest explanatory power). Using all above features at once would make the number of variables too large for this dataset and would also introduce too much correlations between features.\n",
    "\n",
    "You can also prepare several versions of the prepare_users_df method and test which works best in your recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "robust-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_term_WinterVacation', 'user_term_Easter', 'user_term_OffSeason', 'user_term_HighSeason', 'user_term_LowSeason', 'user_term_MayLongWeekend', 'user_term_NewYear', 'user_term_Christmas', 'user_length_of_stay_bucket_[0-1]', 'user_length_of_stay_bucket_[2-3]', 'user_length_of_stay_bucket_[4-7]', 'user_length_of_stay_bucket_[8-inf]', 'user_rate_plan_Standard', 'user_rate_plan_Nonref', 'user_room_segment_[0-160]', 'user_room_segment_[160-260]', 'user_room_segment_[260-360]', 'user_room_segment_[360-500]', 'user_room_segment_[500-900]', 'user_n_people_bucket_[1-1]', 'user_n_people_bucket_[2-2]', 'user_n_people_bucket_[3-4]', 'user_n_people_bucket_[5-inf]', 'user_weekend_stay_True', 'user_weekend_stay_False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_term_WinterVacation</th>\n",
       "      <th>user_term_Easter</th>\n",
       "      <th>user_term_OffSeason</th>\n",
       "      <th>user_term_HighSeason</th>\n",
       "      <th>user_term_LowSeason</th>\n",
       "      <th>user_term_MayLongWeekend</th>\n",
       "      <th>user_term_NewYear</th>\n",
       "      <th>user_term_Christmas</th>\n",
       "      <th>user_length_of_stay_bucket_[0-1]</th>\n",
       "      <th>...</th>\n",
       "      <th>user_room_segment_[160-260]</th>\n",
       "      <th>user_room_segment_[260-360]</th>\n",
       "      <th>user_room_segment_[360-500]</th>\n",
       "      <th>user_room_segment_[500-900]</th>\n",
       "      <th>user_n_people_bucket_[1-1]</th>\n",
       "      <th>user_n_people_bucket_[2-2]</th>\n",
       "      <th>user_n_people_bucket_[3-4]</th>\n",
       "      <th>user_n_people_bucket_[5-inf]</th>\n",
       "      <th>user_weekend_stay_True</th>\n",
       "      <th>user_weekend_stay_False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>50</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>706</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511905</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>1736</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.586207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2785</th>\n",
       "      <td>96</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <td>7779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  user_term_WinterVacation  user_term_Easter  \\\n",
       "0            1                  0.090909               0.0   \n",
       "40          50                  0.043478               0.0   \n",
       "321        706                  0.095238               0.0   \n",
       "547        115                  0.000000               0.0   \n",
       "1270      1736                  0.034483               0.0   \n",
       "2785        96                  0.090909               0.0   \n",
       "11527     7779                  0.000000               0.0   \n",
       "\n",
       "       user_term_OffSeason  user_term_HighSeason  user_term_LowSeason  \\\n",
       "0                 0.681818              0.090909             0.136364   \n",
       "40                0.434783              0.304348             0.217391   \n",
       "321               0.511905              0.190476             0.142857   \n",
       "547               1.000000              0.000000             0.000000   \n",
       "1270              0.482759              0.206897             0.275862   \n",
       "2785              0.681818              0.136364             0.045455   \n",
       "11527             0.500000              0.000000             0.500000   \n",
       "\n",
       "       user_term_MayLongWeekend  user_term_NewYear  user_term_Christmas  \\\n",
       "0                      0.000000           0.000000                  0.0   \n",
       "40                     0.000000           0.000000                  0.0   \n",
       "321                    0.047619           0.011905                  0.0   \n",
       "547                    0.000000           0.000000                  0.0   \n",
       "1270                   0.000000           0.000000                  0.0   \n",
       "2785                   0.045455           0.000000                  0.0   \n",
       "11527                  0.000000           0.000000                  0.0   \n",
       "\n",
       "       user_length_of_stay_bucket_[0-1]  ...  user_room_segment_[160-260]  \\\n",
       "0                              0.000000  ...                     0.863636   \n",
       "40                             0.000000  ...                     0.565217   \n",
       "321                            0.297619  ...                     0.857143   \n",
       "547                            0.000000  ...                     0.500000   \n",
       "1270                           0.241379  ...                     0.931034   \n",
       "2785                           0.272727  ...                     0.863636   \n",
       "11527                          0.000000  ...                     1.000000   \n",
       "\n",
       "       user_room_segment_[260-360]  user_room_segment_[360-500]  \\\n",
       "0                         0.136364                          0.0   \n",
       "40                        0.434783                          0.0   \n",
       "321                       0.107143                          0.0   \n",
       "547                       0.500000                          0.0   \n",
       "1270                      0.068966                          0.0   \n",
       "2785                      0.090909                          0.0   \n",
       "11527                     0.000000                          0.0   \n",
       "\n",
       "       user_room_segment_[500-900]  user_n_people_bucket_[1-1]  \\\n",
       "0                              0.0                    0.000000   \n",
       "40                             0.0                    0.000000   \n",
       "321                            0.0                    0.130952   \n",
       "547                            0.0                    0.500000   \n",
       "1270                           0.0                    0.379310   \n",
       "2785                           0.0                    0.045455   \n",
       "11527                          0.0                    0.000000   \n",
       "\n",
       "       user_n_people_bucket_[2-2]  user_n_people_bucket_[3-4]  \\\n",
       "0                        0.727273                    0.181818   \n",
       "40                       0.173913                    0.521739   \n",
       "321                      0.154762                    0.583333   \n",
       "547                      0.000000                    0.000000   \n",
       "1270                     0.413793                    0.206897   \n",
       "2785                     0.272727                    0.590909   \n",
       "11527                    0.000000                    0.750000   \n",
       "\n",
       "       user_n_people_bucket_[5-inf]  user_weekend_stay_True  \\\n",
       "0                          0.090909                0.636364   \n",
       "40                         0.304348                0.739130   \n",
       "321                        0.130952                0.464286   \n",
       "547                        0.500000                0.000000   \n",
       "1270                       0.000000                0.413793   \n",
       "2785                       0.090909                0.409091   \n",
       "11527                      0.250000                0.750000   \n",
       "\n",
       "       user_weekend_stay_False  \n",
       "0                     0.363636  \n",
       "40                    0.260870  \n",
       "321                   0.535714  \n",
       "547                   1.000000  \n",
       "1270                  0.586207  \n",
       "2785                  0.590909  \n",
       "11527                 0.250000  \n",
       "\n",
       "[7 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from feature_engineering.user_fe import UserFE\n",
    "\n",
    "def prepare_users_df(interactions_df: pd.DataFrame) -> (pd.DataFrame, list):\n",
    "    users_df = interactions_df.copy()\n",
    "    user_fe = UserFE()\n",
    "    \n",
    "    # users_df = user_fe.bucket_features_to_average(users_df)\n",
    "    users_df = user_fe.features_distribution(users_df)\n",
    "    user_features = user_fe.user_features\n",
    "    \n",
    "    users_df = users_df[['user_id'] + user_features]\n",
    "    users_df = users_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return users_df, user_features\n",
    "    \n",
    "\n",
    "users_df, user_features = prepare_users_df(interactions_df)\n",
    "\n",
    "print(user_features)\n",
    "\n",
    "display(users_df.loc[users_df['user_id'].isin([706, 1736, 7779, 96, 1, 50, 115])].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-mediterranean",
   "metadata": {},
   "source": [
    "I decided to use feature distribution per `user_id` as final user features. I was also trying to average numerical features, however after further evaluation in XGBoost/CatBoost feature importances, they haven't as much impact as distribution features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "built-complaint",
   "metadata": {},
   "source": [
    "# Prepare numerical item features\n",
    "\n",
    "<span style=\"color:red\"><font size=\"4\">**Task:**</font></span><br> \n",
    "Code the prepare_items_df method which will be used in the recommender fit and recommend methods to map items to numerical features. This method should take the interactions_df DataFrame as input and return a DataFrame containing one record per item_id with item_id column and numerical item feature columns.\n",
    "\n",
    "You can try turning all item features into one-hot representations. You can use the get_dummies method from pandas. It will return the same columns on any dataset of interactions because the categorical variables with all possible values have been defined in the second cell in this notebook.\n",
    "\n",
    "You are welcome to design your own numerical item features, for instance based on numerical min and max values in buckets used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "formal-munich",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['term_WinterVacation', 'term_Easter', 'term_OffSeason', 'term_HighSeason', 'term_LowSeason', 'term_MayLongWeekend', 'term_NewYear', 'term_Christmas', 'length_of_stay_bucket_[0-1]', 'length_of_stay_bucket_[2-3]', 'length_of_stay_bucket_[4-7]', 'length_of_stay_bucket_[8-inf]', 'rate_plan_Standard', 'rate_plan_Nonref', 'room_segment_[0-160]', 'room_segment_[160-260]', 'room_segment_[260-360]', 'room_segment_[360-500]', 'room_segment_[500-900]', 'n_people_bucket_[1-1]', 'n_people_bucket_[2-2]', 'n_people_bucket_[3-4]', 'n_people_bucket_[5-inf]', 'weekend_stay_True', 'weekend_stay_False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>term_WinterVacation</th>\n",
       "      <th>term_Easter</th>\n",
       "      <th>term_OffSeason</th>\n",
       "      <th>term_HighSeason</th>\n",
       "      <th>term_LowSeason</th>\n",
       "      <th>term_MayLongWeekend</th>\n",
       "      <th>term_NewYear</th>\n",
       "      <th>term_Christmas</th>\n",
       "      <th>length_of_stay_bucket_[0-1]</th>\n",
       "      <th>...</th>\n",
       "      <th>room_segment_[160-260]</th>\n",
       "      <th>room_segment_[260-360]</th>\n",
       "      <th>room_segment_[360-500]</th>\n",
       "      <th>room_segment_[500-900]</th>\n",
       "      <th>n_people_bucket_[1-1]</th>\n",
       "      <th>n_people_bucket_[2-2]</th>\n",
       "      <th>n_people_bucket_[3-4]</th>\n",
       "      <th>n_people_bucket_[5-inf]</th>\n",
       "      <th>weekend_stay_True</th>\n",
       "      <th>weekend_stay_False</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id  term_WinterVacation  term_Easter  term_OffSeason  term_HighSeason  \\\n",
       "0        0                    1            0               0                0   \n",
       "1        1                    1            0               0                0   \n",
       "2        2                    1            0               0                0   \n",
       "3        3                    1            0               0                0   \n",
       "4        4                    1            0               0                0   \n",
       "5        5                    0            1               0                0   \n",
       "6        6                    0            0               1                0   \n",
       "\n",
       "   term_LowSeason  term_MayLongWeekend  term_NewYear  term_Christmas  \\\n",
       "0               0                    0             0               0   \n",
       "1               0                    0             0               0   \n",
       "2               0                    0             0               0   \n",
       "3               0                    0             0               0   \n",
       "4               0                    0             0               0   \n",
       "5               0                    0             0               0   \n",
       "6               0                    0             0               0   \n",
       "\n",
       "   length_of_stay_bucket_[0-1]  ...  room_segment_[160-260]  \\\n",
       "0                            0  ...                       0   \n",
       "1                            0  ...                       1   \n",
       "2                            0  ...                       1   \n",
       "3                            0  ...                       1   \n",
       "4                            0  ...                       0   \n",
       "5                            0  ...                       0   \n",
       "6                            0  ...                       0   \n",
       "\n",
       "   room_segment_[260-360]  room_segment_[360-500]  room_segment_[500-900]  \\\n",
       "0                       1                       0                       0   \n",
       "1                       0                       0                       0   \n",
       "2                       0                       0                       0   \n",
       "3                       0                       0                       0   \n",
       "4                       0                       0                       0   \n",
       "5                       1                       0                       0   \n",
       "6                       1                       0                       0   \n",
       "\n",
       "   n_people_bucket_[1-1]  n_people_bucket_[2-2]  n_people_bucket_[3-4]  \\\n",
       "0                      0                      0                      0   \n",
       "1                      0                      0                      1   \n",
       "2                      0                      1                      0   \n",
       "3                      0                      0                      1   \n",
       "4                      0                      1                      0   \n",
       "5                      0                      0                      0   \n",
       "6                      0                      0                      0   \n",
       "\n",
       "   n_people_bucket_[5-inf]  weekend_stay_True  weekend_stay_False  \n",
       "0                        1                  0                   1  \n",
       "1                        0                  1                   0  \n",
       "2                        0                  0                   1  \n",
       "3                        0                  1                   0  \n",
       "4                        0                  1                   0  \n",
       "5                        1                  1                   0  \n",
       "6                        1                  1                   0  \n",
       "\n",
       "[7 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_items_df(interactions_df: pd.DataFrame, method: str = 'one-hot') -> (pd.DataFrame, list):\n",
    "    \"\"\"\n",
    "    Avaliable methods:\n",
    "    - one-hot: classic one-hot encoding\n",
    "    - ordinal: ordinal encoding, because some of the categorical features have natural order\n",
    "    - catboost: preparation for native CatBoost categorical encoding\n",
    "    - xgboost: ...\n",
    "    - lightgbm: ...\n",
    "    \"\"\"\n",
    "    items_df = interactions_df.copy()\n",
    "    \n",
    "    if method == 'one-hot': \n",
    "        items_df = pd.get_dummies(items_df[['item_id'] + base_item_features])\n",
    "        item_features = items_df.columns.tolist()\n",
    "        item_features.remove('item_id')\n",
    "        \n",
    "    \n",
    "    if method == 'catboost':\n",
    "        items_df = items_df[['item_id'] + base_item_features]\n",
    "        item_features = items_df.columns.tolist()\n",
    "        item_features.remove('item_id')\n",
    "\n",
    "    items_df = items_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return items_df, item_features\n",
    "\n",
    "\n",
    "items_df, item_features = prepare_items_df(interactions_df)\n",
    "\n",
    "print(item_features)\n",
    "display(items_df.loc[items_df['item_id'].isin([0, 1, 2, 3, 4, 5, 6])].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-flesh",
   "metadata": {},
   "source": [
    "For item features we have two main options, first is classic one-hot encoding and second is to leave features as it is and let CatBoost encode them using its native categorical features encoding. All of the item features are categorical features, that is why I decided to try CatBoost, as it may perform better than other GBDT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-imaging",
   "metadata": {},
   "source": [
    "# Content-based recommender\n",
    "\n",
    "<span style=\"color:red\"><font size=\"4\">**Task:**</font></span><br> \n",
    "Code the content-based recommender. User features should be calculated within the fit method based on available training data and should be saved in the object as self.users_df for later use in the recommend method. Item features should be calculated both in the fit method (from interactions_df) and in the recommend method (from items_df - the items to be evaluated).\n",
    "\n",
    "In the fit method you have to randomly generate non-existing interactions and add them to the training data for the regressor. You should add the target variable to interactions - equal to 1 for real (\"positive\") interactions and equal to 0 for those newly added \"negative\" interactions. Generate several negative interactions per every positive interaction (n_neg_per_pos). Treat the proportion as a tunable parameter of the model.\n",
    "\n",
    "Remember to keep control over randomness - in the init method add seed as a parameter and initialize the random seed generator with that seed:\n",
    "\n",
    "```python\n",
    "self.seed = seed\n",
    "self.rng = np.random.RandomState(seed=seed)\n",
    "```\n",
    "\n",
    "Below the base content-based recommender class there are several classes which inherit from the base class and use different ML models:\n",
    "  - LinearRegressionCBUIRecommender - based on linear regression,\n",
    "  - SVRCBUIRecommender - based on Support Vector Regressor (if you want to test it, sample the data in the fit method, as the training can take many hours on the entire dataset of interactions),\n",
    "  - RandomForestCBUIRecommender - based on Random Forest,\n",
    "  - XGBoostCBUIRecommender - based on XGBoost.\n",
    "  \n",
    "There is no need to change anything in those inheriting classes, although you can experiment with other tunable parameters of the underlying models.\n",
    "\n",
    "You are encouraged to experiment with:\n",
    "  - Other numerical user and item features (but always train and evaluate the model on buckets defined in the first notebook).\n",
    "  - Other ML models, e.g. Huber regression, Lasso regression, Ridge regression, LARS regression, Linear SVR, Decision Tree, Naive Bayes, LightGBM, Neural Networks or any model of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "unlike-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from recommenders.recommender import Recommender\n",
    "\n",
    "\n",
    "class ContentBasedUserItemRecommender(Recommender):\n",
    "    \"\"\"\n",
    "    Linear recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        self.model = LinearRegression()\n",
    "        self.n_neg_per_pos = n_neg_per_pos\n",
    "        \n",
    "        self.recommender_df = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        self.users_df = None\n",
    "        self.user_features = None\n",
    "        self.model_features = None\n",
    "        \n",
    "        self.seed = seed\n",
    "        self.rng = np.random.RandomState(seed=seed)\n",
    "    \n",
    "    def fit(self, interactions_df, users_df, items_df):\n",
    "        \"\"\"\n",
    "        Training of the recommender.\n",
    "        \n",
    "        :param pd.DataFrame interactions_df: DataFrame with recorded interactions between users and items \n",
    "            defined by user_id, item_id and features of the interaction.\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features defined by user_id and the user feature columns.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features defined by item_id and the item feature columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        interactions_df = interactions_df.copy()\n",
    "        \n",
    "        # Prepare users_df and items_df\n",
    "        \n",
    "        users_df, user_features = prepare_users_df(interactions_df)\n",
    "        \n",
    "        self.users_df = users_df\n",
    "        self.user_features = user_features\n",
    "        \n",
    "        items_df, item_features = prepare_items_df(interactions_df)\n",
    "        items_df = items_df.loc[:, ['item_id'] + item_features]\n",
    "        \n",
    "        # Generate negative interactions\n",
    "        \n",
    "        interactions_df = interactions_df.loc[:, ['user_id', 'item_id']]\n",
    "        \n",
    "        interactions_df.loc[:, 'interacted'] = 1\n",
    "                \n",
    "        negative_interactions = []\n",
    "        \n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        # Generate tuples (user_id, item_id, 0) for pairs (user_id, item_id) which do not\n",
    "        # appear in the interactions_df and add those tuples to the list negative_interactions.\n",
    "        # Generate self.n_neg_per_pos * len(interactions_df) negative interactions \n",
    "        # (self.n_neg_per_pos per one positive).\n",
    "        # Make sure the code is efficient and runs fast, otherwise you will not be able to properly tune your model.\n",
    "        \n",
    "        # create interactions dictionary for O(1) time complexity\n",
    "        interactions_dict = interactions_df.set_index(['user_id', 'item_id'])['interacted'].to_dict()\n",
    "        n_items = interactions_df['item_id'].nunique()\n",
    "        n_users = interactions_df['user_id'].nunique()\n",
    "        curr_generated = 0\n",
    "        \n",
    "        while curr_generated < self.n_neg_per_pos * len(interactions_df):\n",
    "            random_user_id = np.random.randint(0, n_users)\n",
    "            random_item_id = np.random.randint(0, n_items)\n",
    "            # print(f\"iteration: {curr_generated} item_id: {random_item_id} user_id: {random_user_id}\")\n",
    "            \n",
    "            if (random_user_id, random_item_id) not in interactions_dict:\n",
    "                    negative_interactions.append((random_user_id, random_item_id, 0))\n",
    "                    curr_generated += 1\n",
    "        \n",
    "        interactions_df = pd.concat(\n",
    "            [interactions_df, pd.DataFrame(negative_interactions, columns=['user_id', 'item_id', 'interacted'])])\n",
    "                \n",
    "        # Get the input data for the model\n",
    "        \n",
    "        interactions_df = pd.merge(interactions_df, users_df, on=['user_id'])\n",
    "        interactions_df = pd.merge(interactions_df, items_df, on=['item_id'])\n",
    "        \n",
    "        self.model_features = user_features + item_features\n",
    "        X = interactions_df.loc[:, self.model_features]\n",
    "        y = interactions_df['interacted']\n",
    "        \n",
    "        # saving for EDA purpose\n",
    "        # interactions_df.to_csv(\"interactions_df.csv\", index=False)\n",
    "        \n",
    "        # print(\"TRAINING...\")\n",
    "        \n",
    "        self.model.fit(X.values, y.values)\n",
    "        # self.model.fit(X, y, cat_features=item_features)\n",
    "        \n",
    "    \n",
    "    def recommend(self, users_df, items_df, n_recommendations=1):\n",
    "        \"\"\"\n",
    "        Serving of recommendations. Scores items in items_df for each user in users_df and returns \n",
    "        top n_recommendations for each user.\n",
    "        \n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features for which recommendations should be generated.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features which should be scored.\n",
    "        :param int n_recommendations: Number of recommendations to be returned for each user.\n",
    "        :return: DataFrame with user_id, item_id and score as columns returning n_recommendations top recommendations \n",
    "            for each user.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # Clean previous recommendations (iloc could be used alternatively)\n",
    "        self.recommender_df = self.recommender_df[:0]\n",
    "        \n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        # Prepare users_df and items_df\n",
    "        # For users_df you need to merge user features from self.users_df to users_df \n",
    "        # (the users for which you generate recommendations).\n",
    "        # Note that for users who were not in the hotel before (which is true for most users)\n",
    "        # there will be no features in self.users_df. For such users you can initialize their features\n",
    "        # with all zeros (for instance with fillna(0)), but you can also try to use average feature\n",
    "        # values from self.users_df (this way you would trear a new user as an average user).\n",
    "        # For items you have to apply the prepare_items_df method to items_df.\n",
    "        \n",
    "        # Score the users\n",
    "        users_df = pd.merge(users_df, self.users_df, on=['user_id'], how='left')\n",
    "        users_df = users_df.fillna(self.users_df.mean())\n",
    "        \n",
    "        # Score the items\n",
    "        items_df, item_features = prepare_items_df(items_df)\n",
    "        \n",
    "    \n",
    "        recommendations = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        \n",
    "        for ix, user in users_df.iterrows():\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Create a Carthesian product of users from users_df and items from items_df\n",
    "            product = pd.merge(user.to_frame().T, items_df, how='cross')\n",
    "            product = product.drop(columns='item_id')\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Use self.model.predict method to calculate scores for all records in the just created DataFrame\n",
    "            # of users and items\n",
    "            scores = self.model.predict(product.loc[:, self.model_features].values)\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Obtain item ids with the highest score and save those ids under the chosen_ids variable\n",
    "            # Do not exclude already booked items.\n",
    "            chosen_ids = np.argsort(-scores)[:n_recommendations]\n",
    "            \n",
    "            recommendations = []\n",
    "            for item_id in chosen_ids:\n",
    "                recommendations.append(\n",
    "                    {\n",
    "                        'user_id': user['user_id'],\n",
    "                        'item_id': item_id,\n",
    "                        'score': scores[item_id]\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            user_recommendations = pd.DataFrame(recommendations)\n",
    "\n",
    "            self.recommender_df = pd.concat([self.recommender_df, user_recommendations])\n",
    "\n",
    "        return self.recommender_df\n",
    "    \n",
    "    \n",
    "class LinearRegressionCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    Linear regression recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        self.model = LinearRegression()\n",
    "        \n",
    "        \n",
    "class SVRCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    SVR recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        if 'kernel' in model_params:\n",
    "            self.kernel = model_params['kernel']\n",
    "        else:\n",
    "            self.kernel = 'rbf'\n",
    "        if 'C' in model_params:\n",
    "            self.C = model_params['C']\n",
    "        else:\n",
    "            self.C = 1.0\n",
    "        if 'gamma' in model_params:\n",
    "            self.gamma = model_params['gamma']\n",
    "        else:\n",
    "            self.gamma = 'scale'\n",
    "        if 'epsilon' in model_params:\n",
    "            self.epsilon = model_params['epsilon']\n",
    "        else:\n",
    "            self.epsilon = 0.1\n",
    "        self.model = SVR(kernel=self.kernel, C=self.C, epsilon=self.epsilon)\n",
    "        \n",
    "    \n",
    "class RandomForestCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    Random forest recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        if 'n_estimators' in model_params:\n",
    "            self.n_estimators = int(model_params['n_estimators'])\n",
    "        else:\n",
    "            self.n_estimators = 100\n",
    "        if 'max_depth' in model_params:\n",
    "            self.max_depth = int(model_params['max_depth'])\n",
    "        else:\n",
    "            self.max_depth = 30\n",
    "        if 'min_samples_split' in model_params:\n",
    "            self.min_samples_split = int(model_params['min_samples_split'])\n",
    "        else:\n",
    "            self.min_samples_split = 30\n",
    "        self.model = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators, max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "    \n",
    "    \n",
    "class XGBoostCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    XGBoost recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        if 'n_estimators' in model_params:\n",
    "            self.n_estimators = int(model_params['n_estimators'])\n",
    "        else:\n",
    "            self.n_estimators = 100\n",
    "        if 'max_depth' in model_params:\n",
    "            self.max_depth = int(model_params['max_depth'])\n",
    "        else:\n",
    "            self.max_depth = 30\n",
    "        if 'min_samples_split' in model_params:\n",
    "            self.min_samples_split = int(model_params['min_samples_split'])\n",
    "        else:\n",
    "            self.min_samples_split = 30\n",
    "        if 'learning_rate' in model_params:\n",
    "            self.learning_rate = model_params['learning_rate']\n",
    "        else:\n",
    "            self.learning_rate = 30\n",
    "        self.model = GradientBoostingRegressor(\n",
    "            n_estimators=self.n_estimators, max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "            learning_rate=self.learning_rate)    \n",
    "        \n",
    "        \n",
    "class CatBoostCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    CatBoost recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        self.model = CatBoostRegressor(**model_params, verbose=0)\n",
    "        \n",
    "        \n",
    "class LogisticRegressionCBUIRecommender(ContentBasedUserItemRecommender):\n",
    "    \"\"\"\n",
    "    Linear regression recommender class based on user and item features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=6789, n_neg_per_pos=5, **model_params):\n",
    "        \"\"\"\n",
    "        Initialize base recommender params and variables.\n",
    "        \"\"\"\n",
    "        super().__init__(seed=seed, n_neg_per_pos=n_neg_per_pos)\n",
    "        self.model = LogisticRegression(**model_params)\n",
    "        \n",
    "    def fit(self, interactions_df, users_df, items_df):\n",
    "        \"\"\"\n",
    "        Training of the recommender.\n",
    "        \n",
    "        :param pd.DataFrame interactions_df: DataFrame with recorded interactions between users and items \n",
    "            defined by user_id, item_id and features of the interaction.\n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features defined by user_id and the user feature columns.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features defined by item_id and the item feature columns.\n",
    "        \"\"\"\n",
    "        \n",
    "        interactions_df = interactions_df.copy()\n",
    "        \n",
    "        # Prepare users_df and items_df\n",
    "        \n",
    "        users_df, user_features = prepare_users_df(interactions_df)\n",
    "        \n",
    "        self.users_df = users_df\n",
    "        self.user_features = user_features\n",
    "        \n",
    "        items_df, item_features = prepare_items_df(interactions_df)\n",
    "        items_df = items_df.loc[:, ['item_id'] + item_features]\n",
    "        \n",
    "        # Generate negative interactions\n",
    "        \n",
    "        interactions_df = interactions_df.loc[:, ['user_id', 'item_id']]\n",
    "        \n",
    "        interactions_df.loc[:, 'interacted'] = 1\n",
    "        \n",
    "        # display(interactions_df)\n",
    "        # display(items_df)\n",
    "        # display(users_df)\n",
    "                \n",
    "        negative_interactions = []\n",
    "        \n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        # Generate tuples (user_id, item_id, 0) for pairs (user_id, item_id) which do not\n",
    "        # appear in the interactions_df and add those tuples to the list negative_interactions.\n",
    "        # Generate self.n_neg_per_pos * len(interactions_df) negative interactions \n",
    "        # (self.n_neg_per_pos per one positive).\n",
    "        # Make sure the code is efficient and runs fast, otherwise you will not be able to properly tune your model.\n",
    "        \n",
    "        # create interactions dictionary for O(1) time complexity\n",
    "        interactions_dict = interactions_df.set_index(['user_id', 'item_id'])['interacted'].to_dict()\n",
    "        n_items = interactions_df['item_id'].nunique()\n",
    "        n_users = interactions_df['user_id'].nunique()\n",
    "        curr_generated = 0\n",
    "        \n",
    "        while curr_generated < self.n_neg_per_pos * len(interactions_df):\n",
    "            random_user_id = np.random.randint(0, n_users)\n",
    "            random_item_id = np.random.randint(0, n_items)\n",
    "            # print(f\"iteration: {curr_generated} item_id: {random_item_id} user_id: {random_user_id}\")\n",
    "            \n",
    "            if (random_user_id, random_item_id) not in interactions_dict:\n",
    "                    negative_interactions.append((random_user_id, random_item_id, 0))\n",
    "                    curr_generated += 1\n",
    "        \n",
    "        interactions_df = pd.concat(\n",
    "            [interactions_df, pd.DataFrame(negative_interactions, columns=['user_id', 'item_id', 'interacted'])])\n",
    "                \n",
    "        # Get the input data for the model\n",
    "        \n",
    "        interactions_df = pd.merge(interactions_df, users_df, on=['user_id'])\n",
    "        interactions_df = pd.merge(interactions_df, items_df, on=['item_id'])\n",
    "        \n",
    "        self.model_features = user_features + item_features\n",
    "        X = interactions_df.loc[:, self.model_features]\n",
    "        y = interactions_df['interacted']\n",
    "        \n",
    "        # print(\"TRAINING...\")\n",
    "        \n",
    "        # display(X.columns)\n",
    "        self.model.fit(X.values, y.values)\n",
    "        \n",
    "    \n",
    "    def recommend(self, users_df, items_df, n_recommendations=1):\n",
    "        \"\"\"\n",
    "        Serving of recommendations. Scores items in items_df for each user in users_df and returns \n",
    "        top n_recommendations for each user.\n",
    "        \n",
    "        :param pd.DataFrame users_df: DataFrame with users and their features for which recommendations should be generated.\n",
    "        :param pd.DataFrame items_df: DataFrame with items and their features which should be scored.\n",
    "        :param int n_recommendations: Number of recommendations to be returned for each user.\n",
    "        :return: DataFrame with user_id, item_id and score as columns returning n_recommendations top recommendations \n",
    "            for each user.\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # Clean previous recommendations (iloc could be used alternatively)\n",
    "        self.recommender_df = self.recommender_df[:0]\n",
    "        \n",
    "        ########################\n",
    "        # Write your code here #\n",
    "        # Prepare users_df and items_df\n",
    "        # For users_df you need to merge user features from self.users_df to users_df \n",
    "        # (the users for which you generate recommendations).\n",
    "        # Note that for users who were not in the hotel before (which is true for most users)\n",
    "        # there will be no features in self.users_df. For such users you can initialize their features\n",
    "        # with all zeros (for instance with fillna(0)), but you can also try to use average feature\n",
    "        # values from self.users_df (this way you would trear a new user as an average user).\n",
    "        # For items you have to apply the prepare_items_df method to items_df.\n",
    "        \n",
    "        # Score the users\n",
    "        users_df = pd.merge(users_df, self.users_df, on=['user_id'], how='left')\n",
    "        users_df = users_df.fillna(self.users_df.mean()) # TODO: mean encoding, most popular encoding\n",
    "        \n",
    "        # Score the items\n",
    "        items_df, item_features = prepare_items_df(items_df)\n",
    "        \n",
    "    \n",
    "        recommendations = pd.DataFrame(columns=['user_id', 'item_id', 'score'])\n",
    "        \n",
    "        for ix, user in users_df.iterrows():\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Create a Carthesian product of users from users_df and items from items_df\n",
    "            product = pd.merge(user.to_frame().T, items_df, how='cross')\n",
    "            product = product.drop(columns='item_id')\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Use self.model.predict method to calculate scores for all records in the just created DataFrame\n",
    "            # of users and items\n",
    "            scores = self.model.predict_proba(product.loc[:, self.model_features].values)[:, 1]\n",
    "            \n",
    "            ########################\n",
    "            # Write your code here #\n",
    "            # Obtain item ids with the highest score and save those ids under the chosen_ids variable\n",
    "            # Do not exclude already booked items.\n",
    "            chosen_ids = np.argsort(-scores)[:n_recommendations]\n",
    "            \n",
    "            recommendations = []\n",
    "            for item_id in chosen_ids:\n",
    "                recommendations.append(\n",
    "                    {\n",
    "                        'user_id': user['user_id'],\n",
    "                        'item_id': item_id,\n",
    "                        'score': scores[item_id]\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            user_recommendations = pd.DataFrame(recommendations)\n",
    "\n",
    "            self.recommender_df = pd.concat([self.recommender_df, user_recommendations])\n",
    "\n",
    "        return self.recommender_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-relative",
   "metadata": {},
   "source": [
    "# Quick test of the recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "greatest-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_df = interactions_df.loc[:, ['item_id'] + base_item_features].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "initial-capital",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit method\n",
    "cb_user_item_recommender = LinearRegressionCBUIRecommender()\n",
    "cb_user_item_recommender.fit(interactions_df, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "digital-consolidation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>score</th>\n",
       "      <th>term</th>\n",
       "      <th>length_of_stay_bucket</th>\n",
       "      <th>rate_plan</th>\n",
       "      <th>room_segment</th>\n",
       "      <th>n_people_bucket</th>\n",
       "      <th>weekend_stay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.586823</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.576172</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.568756</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.558105</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.548187</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.537537</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.530121</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.519470</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.481903</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.477936</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.597809</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.587158</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.579498</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.568848</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.559174</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.548523</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.540863</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.530212</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.492889</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.488922</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.606354</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.595703</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.588043</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.577393</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.567719</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.557068</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.549408</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.538757</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.501434</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3.0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.497467</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.615875</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.605225</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.597565</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.586914</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.577240</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.566589</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.558929</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.548279</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.510956</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.506989</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>5.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.666901</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>5.0</td>\n",
       "      <td>56</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>5.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.648346</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>5.0</td>\n",
       "      <td>33</td>\n",
       "      <td>0.637695</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>5.0</td>\n",
       "      <td>57</td>\n",
       "      <td>0.628265</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.617615</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.609711</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5.0</td>\n",
       "      <td>21</td>\n",
       "      <td>0.599060</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Standard</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[2-2]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.561981</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[2-3]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[5-inf]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.0</td>\n",
       "      <td>78</td>\n",
       "      <td>0.557770</td>\n",
       "      <td>OffSeason</td>\n",
       "      <td>[4-7]</td>\n",
       "      <td>Nonref</td>\n",
       "      <td>[160-260]</td>\n",
       "      <td>[3-4]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id item_id     score       term length_of_stay_bucket rate_plan  \\\n",
       "0       1.0      52  0.586823  OffSeason                 [2-3]    Nonref   \n",
       "1       1.0      56  0.576172  OffSeason                 [2-3]    Nonref   \n",
       "2       1.0      22  0.568756  OffSeason                 [2-3]  Standard   \n",
       "3       1.0      33  0.558105  OffSeason                 [2-3]  Standard   \n",
       "4       1.0      57  0.548187  OffSeason                 [2-3]    Nonref   \n",
       "5       1.0      54  0.537537  OffSeason                 [2-3]    Nonref   \n",
       "6       1.0      31  0.530121  OffSeason                 [2-3]  Standard   \n",
       "7       1.0      21  0.519470  OffSeason                 [2-3]  Standard   \n",
       "8       1.0      63  0.481903  OffSeason                 [2-3]    Nonref   \n",
       "9       1.0      78  0.477936  OffSeason                 [4-7]    Nonref   \n",
       "10      2.0      52  0.597809  OffSeason                 [2-3]    Nonref   \n",
       "11      2.0      56  0.587158  OffSeason                 [2-3]    Nonref   \n",
       "12      2.0      22  0.579498  OffSeason                 [2-3]  Standard   \n",
       "13      2.0      33  0.568848  OffSeason                 [2-3]  Standard   \n",
       "14      2.0      57  0.559174  OffSeason                 [2-3]    Nonref   \n",
       "15      2.0      54  0.548523  OffSeason                 [2-3]    Nonref   \n",
       "16      2.0      31  0.540863  OffSeason                 [2-3]  Standard   \n",
       "17      2.0      21  0.530212  OffSeason                 [2-3]  Standard   \n",
       "18      2.0      63  0.492889  OffSeason                 [2-3]    Nonref   \n",
       "19      2.0      78  0.488922  OffSeason                 [4-7]    Nonref   \n",
       "20      3.0      52  0.606354  OffSeason                 [2-3]    Nonref   \n",
       "21      3.0      56  0.595703  OffSeason                 [2-3]    Nonref   \n",
       "22      3.0      22  0.588043  OffSeason                 [2-3]  Standard   \n",
       "23      3.0      33  0.577393  OffSeason                 [2-3]  Standard   \n",
       "24      3.0      57  0.567719  OffSeason                 [2-3]    Nonref   \n",
       "25      3.0      54  0.557068  OffSeason                 [2-3]    Nonref   \n",
       "26      3.0      31  0.549408  OffSeason                 [2-3]  Standard   \n",
       "27      3.0      21  0.538757  OffSeason                 [2-3]  Standard   \n",
       "28      3.0      63  0.501434  OffSeason                 [2-3]    Nonref   \n",
       "29      3.0      78  0.497467  OffSeason                 [4-7]    Nonref   \n",
       "30      4.0      52  0.615875  OffSeason                 [2-3]    Nonref   \n",
       "31      4.0      56  0.605225  OffSeason                 [2-3]    Nonref   \n",
       "32      4.0      22  0.597565  OffSeason                 [2-3]  Standard   \n",
       "33      4.0      33  0.586914  OffSeason                 [2-3]  Standard   \n",
       "34      4.0      57  0.577240  OffSeason                 [2-3]    Nonref   \n",
       "35      4.0      54  0.566589  OffSeason                 [2-3]    Nonref   \n",
       "36      4.0      31  0.558929  OffSeason                 [2-3]  Standard   \n",
       "37      4.0      21  0.548279  OffSeason                 [2-3]  Standard   \n",
       "38      4.0      63  0.510956  OffSeason                 [2-3]    Nonref   \n",
       "39      4.0      78  0.506989  OffSeason                 [4-7]    Nonref   \n",
       "40      5.0      52  0.666901  OffSeason                 [2-3]    Nonref   \n",
       "41      5.0      56  0.656250  OffSeason                 [2-3]    Nonref   \n",
       "42      5.0      22  0.648346  OffSeason                 [2-3]  Standard   \n",
       "43      5.0      33  0.637695  OffSeason                 [2-3]  Standard   \n",
       "44      5.0      57  0.628265  OffSeason                 [2-3]    Nonref   \n",
       "45      5.0      54  0.617615  OffSeason                 [2-3]    Nonref   \n",
       "46      5.0      31  0.609711  OffSeason                 [2-3]  Standard   \n",
       "47      5.0      21  0.599060  OffSeason                 [2-3]  Standard   \n",
       "48      5.0      63  0.561981  OffSeason                 [2-3]    Nonref   \n",
       "49      5.0      78  0.557770  OffSeason                 [4-7]    Nonref   \n",
       "\n",
       "   room_segment n_people_bucket weekend_stay  \n",
       "0     [160-260]           [3-4]         True  \n",
       "1     [160-260]           [3-4]        False  \n",
       "2     [160-260]           [3-4]         True  \n",
       "3     [160-260]           [3-4]        False  \n",
       "4     [160-260]           [2-2]         True  \n",
       "5     [160-260]           [2-2]        False  \n",
       "6     [160-260]           [2-2]         True  \n",
       "7     [160-260]           [2-2]        False  \n",
       "8     [160-260]         [5-inf]         True  \n",
       "9     [160-260]           [3-4]         True  \n",
       "10    [160-260]           [3-4]         True  \n",
       "11    [160-260]           [3-4]        False  \n",
       "12    [160-260]           [3-4]         True  \n",
       "13    [160-260]           [3-4]        False  \n",
       "14    [160-260]           [2-2]         True  \n",
       "15    [160-260]           [2-2]        False  \n",
       "16    [160-260]           [2-2]         True  \n",
       "17    [160-260]           [2-2]        False  \n",
       "18    [160-260]         [5-inf]         True  \n",
       "19    [160-260]           [3-4]         True  \n",
       "20    [160-260]           [3-4]         True  \n",
       "21    [160-260]           [3-4]        False  \n",
       "22    [160-260]           [3-4]         True  \n",
       "23    [160-260]           [3-4]        False  \n",
       "24    [160-260]           [2-2]         True  \n",
       "25    [160-260]           [2-2]        False  \n",
       "26    [160-260]           [2-2]         True  \n",
       "27    [160-260]           [2-2]        False  \n",
       "28    [160-260]         [5-inf]         True  \n",
       "29    [160-260]           [3-4]         True  \n",
       "30    [160-260]           [3-4]         True  \n",
       "31    [160-260]           [3-4]        False  \n",
       "32    [160-260]           [3-4]         True  \n",
       "33    [160-260]           [3-4]        False  \n",
       "34    [160-260]           [2-2]         True  \n",
       "35    [160-260]           [2-2]        False  \n",
       "36    [160-260]           [2-2]         True  \n",
       "37    [160-260]           [2-2]        False  \n",
       "38    [160-260]         [5-inf]         True  \n",
       "39    [160-260]           [3-4]         True  \n",
       "40    [160-260]           [3-4]         True  \n",
       "41    [160-260]           [3-4]        False  \n",
       "42    [160-260]           [3-4]         True  \n",
       "43    [160-260]           [3-4]        False  \n",
       "44    [160-260]           [2-2]         True  \n",
       "45    [160-260]           [2-2]        False  \n",
       "46    [160-260]           [2-2]         True  \n",
       "47    [160-260]           [2-2]        False  \n",
       "48    [160-260]         [5-inf]         True  \n",
       "49    [160-260]           [3-4]         True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recommender method\n",
    "\n",
    "recommendations = cb_user_item_recommender.recommend(pd.DataFrame([[1], [2], [3], [4], [5]], columns=['user_id']), interactions_df, 10)\n",
    "\n",
    "recommendations = pd.merge(recommendations, items_df, on='item_id', how='left')\n",
    "display(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-eleven",
   "metadata": {},
   "source": [
    "# Tuning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "strange-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_and_testing.testing import evaluate_train_test_split_implicit\n",
    "\n",
    "seed = 6789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "stable-theta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import traceback\n",
    "\n",
    "def tune_recommender(recommender_class, interactions_df, items_df, \n",
    "                     param_space, max_evals=1, show_progressbar=True, seed=6789):\n",
    "    # Split into train_validation and test sets\n",
    "\n",
    "    shuffle = np.arange(len(interactions_df))\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    rng.shuffle(shuffle)\n",
    "    shuffle = list(shuffle)\n",
    "\n",
    "    train_test_split = 0.8\n",
    "    split_index = int(len(interactions_df) * train_test_split)\n",
    "\n",
    "    train_validation = interactions_df.iloc[shuffle[:split_index]]\n",
    "    test = interactions_df.iloc[shuffle[split_index:]]\n",
    "\n",
    "    # Tune\n",
    "\n",
    "    def loss(tuned_params):\n",
    "        # tuned_params['n_estimators'] = round(tuned_params['n_estimators'])\n",
    "        recommender = recommender_class(seed=seed, **tuned_params)\n",
    "        hr1, hr3, hr5, hr10, ndcg1, ndcg3, ndcg5, ndcg10 = evaluate_train_test_split_implicit(\n",
    "            recommender, train_validation, items_df, seed=seed)\n",
    "        return -hr10\n",
    "\n",
    "    n_tries = 1\n",
    "    succeded = False\n",
    "    try_id = 0\n",
    "    while not succeded and try_id < n_tries:\n",
    "        try:\n",
    "            trials = Trials()\n",
    "            best_param_set = fmin(loss, space=param_space, algo=tpe.suggest, \n",
    "                                  max_evals=max_evals, show_progressbar=show_progressbar, trials=trials, verbose=True)\n",
    "            succeded = True\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            try_id += 1\n",
    "            \n",
    "    if not succeded:\n",
    "        return None\n",
    "        \n",
    "    # Validate\n",
    "    # best_param_set['n_estimators'] = round(best_param_set['n_estimators'])\n",
    "    recommender = recommender_class(seed=seed, **best_param_set)\n",
    "\n",
    "    results = [[recommender_class.__name__] + list(evaluate_train_test_split_implicit(\n",
    "        recommender, {'train': train_validation, 'test': test}, items_df, seed=seed))]\n",
    "\n",
    "    results = pd.DataFrame(results, \n",
    "                           columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "    display(results)\n",
    "    \n",
    "    return best_param_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-orbit",
   "metadata": {},
   "source": [
    "## Tuning of the recommender\n",
    "\n",
    "<span style=\"color:red\"><font size=\"4\">**Task:**</font></span><br> \n",
    "Tune your models using the code below. You only need to put the class name of your recommender and choose an appropriate parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dependent-capital",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 10/10 [08:25<00:00, 50.53s/trial, best loss: -0.21752010156580617]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegressionCBUIRecommender</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.09131</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.068725</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.115345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Recommender      HR@1     HR@3      HR@5     HR@10  \\\n",
       "0  LinearRegressionCBUIRecommender  0.041073  0.09131  0.145282  0.220638   \n",
       "\n",
       "     NDCG@1    NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.041073  0.068725  0.090407  0.115345  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'n_neg_per_pos': 4.0}\n"
     ]
    }
   ],
   "source": [
    "param_space = {\n",
    "    'n_neg_per_pos': hp.quniform('n_neg_per_pos', 1, 10, 1)\n",
    "}\n",
    "\n",
    "best_param_set = tune_recommender(LinearRegressionCBUIRecommender, interactions_df, items_df,\n",
    "                                  param_space, max_evals=10, show_progressbar=True, seed=seed)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(best_param_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moved-gothic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "TRAINING...                                                                                                            \n",
      "100%|██████████████████████████████████████████| 300/300 [4:28:03<00:00, 53.61s/trial, best loss: -0.23063901819720695]\n",
      "TRAINING...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoostCBUIRecommender</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.076035</td>\n",
       "      <td>0.133401</td>\n",
       "      <td>0.233876</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.058432</td>\n",
       "      <td>0.082409</td>\n",
       "      <td>0.115551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Recommender      HR@1      HR@3      HR@5     HR@10    NDCG@1  \\\n",
       "0  XGBoostCBUIRecommender  0.034963  0.076035  0.133401  0.233876  0.034963   \n",
       "\n",
       "     NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.058432  0.082409  0.115551  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'learning_rate': 0.012153727291261835, 'max_depth': 6.0, 'min_samples_split': 10.0, 'n_estimators': 148.0, 'n_neg_per_pos': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# This tuning may take around 12 hours\n",
    "\n",
    "param_space = {\n",
    "    'n_neg_per_pos': hp.quniform('n_neg_per_pos', 1, 10, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 300, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 2, 10, 1),\n",
    "    'min_samples_split': hp.quniform('min_samples_split', 2, 30, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1))\n",
    "}\n",
    "\n",
    "best_param_set = tune_recommender(XGBoostCBUIRecommender, interactions_df, items_df,\n",
    "                                  param_space, max_evals=300, show_progressbar=True, seed=seed)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(best_param_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "honey-dover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [3:04:39<00:00, 110.80s/trial, best loss: -0.23656369022429116]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostCBUIRecommender</td>\n",
       "      <td>0.051256</td>\n",
       "      <td>0.104549</td>\n",
       "      <td>0.157841</td>\n",
       "      <td>0.244399</td>\n",
       "      <td>0.051256</td>\n",
       "      <td>0.080791</td>\n",
       "      <td>0.103014</td>\n",
       "      <td>0.131365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Recommender      HR@1      HR@3      HR@5     HR@10    NDCG@1  \\\n",
       "0  CatBoostCBUIRecommender  0.051256  0.104549  0.157841  0.244399  0.051256   \n",
       "\n",
       "     NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.080791  0.103014  0.131365  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:\n",
      "{'learning_rate': 0.07358203024749414, 'max_depth': 5.0, 'n_estimators': 1122, 'n_neg_per_pos': 11.0}\n"
     ]
    }
   ],
   "source": [
    "# CatBoostRecommender tuning\n",
    "\n",
    "param_space = {\n",
    "    'n_neg_per_pos': hp.quniform('n_neg_per_pos', 1, 15, 1),\n",
    "    'n_estimators': hp.uniform('n_estimators', 100, 1500),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 10, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1))\n",
    "}\n",
    "\n",
    "best_param_set = tune_recommender(CatBoostCBUIRecommender, interactions_df, items_df,\n",
    "                                  param_space, max_evals=100, show_progressbar=True, seed=seed)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(best_param_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "desperate-variety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params set: {'C': 1, 'gamma': 0.001}\n",
      "Wall time: 9h 46min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# simple GridSearch tuning for SVR, as hyperopt takes too long\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'gamma': [1, 0.1, 0.01, 0.001]\n",
    "}\n",
    "best_hr_10 = 0.0\n",
    "best_params = {}\n",
    "seed = 6789\n",
    "\n",
    "shuffle = np.arange(len(interactions_df))\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "rng.shuffle(shuffle)\n",
    "shuffle = list(shuffle)\n",
    "\n",
    "train_test_split = 0.8\n",
    "split_index = int(len(interactions_df) * train_test_split)\n",
    "\n",
    "train_validation = interactions_df.iloc[shuffle[:split_index]]\n",
    "test = interactions_df.iloc[shuffle[split_index:]]\n",
    "\n",
    "for c in param_grid['C']:\n",
    "    for gamma in param_grid['gamma']:\n",
    "        tuned_params = {'C': c, 'gamma': gamma}\n",
    "        recommender = SVRCBUIRecommender(seed=seed, **tuned_params)\n",
    "        hr1, hr3, hr5, hr10, ndcg1, ndcg3, ndcg5, ndcg10 = evaluate_train_test_split_implicit(\n",
    "            recommender, train_validation, items_df, seed=seed)\n",
    "        \n",
    "        if hr10 > best_hr_10:\n",
    "            best_hr_10 = hr10\n",
    "            best_params = tuned_params\n",
    "\n",
    "\n",
    "print(f\"Best params set: {best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-strap",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "\n",
    "<span style=\"color:red\"><font size=\"4\">**Task:**</font></span><br> \n",
    "Run the final evaluation of your recommender and present its results against the Amazon recommender's results. You can present results for several of your recommenders. You just need to give the class name of your recommender and its tuned parameters below. If you present results for several recommenders, you should add a separate cell for each recommender and change the names of the DataFrames containing results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-shoot",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "given-homework",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegressionCBUIRecommender</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.09131</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.068725</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.115345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Recommender      HR@1     HR@3      HR@5     HR@10  \\\n",
       "0  LinearRegressionCBUIRecommender  0.041073  0.09131  0.145282  0.220638   \n",
       "\n",
       "     NDCG@1    NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.041073  0.068725  0.090407  0.115345  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb_user_item_recommender = LinearRegressionCBUIRecommender(\n",
    "    **{'n_neg_per_pos': 4}) \n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "linear_reg_cbui_tts_results = [['LinearRegressionCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    cb_user_item_recommender, interactions_df, items_df))]\n",
    "\n",
    "linear_reg_cbui_tts_results = pd.DataFrame(\n",
    "    linear_reg_cbui_tts_results,\n",
    "    columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(linear_reg_cbui_tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-penny",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "aggressive-perception",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestCBUIRecommender</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.089613</td>\n",
       "      <td>0.153768</td>\n",
       "      <td>0.239308</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.089649</td>\n",
       "      <td>0.117323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Recommender      HR@1      HR@3      HR@5     HR@10  \\\n",
       "0  RandomForestCBUIRecommender  0.032926  0.089613  0.153768  0.239308   \n",
       "\n",
       "     NDCG@1    NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.032926  0.063581  0.089649  0.117323  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb_user_item_recommender = RandomForestCBUIRecommender(\n",
    "    **{'n_neg_per_pos': 5})\n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "random_forest_cbui_tts_results = [['RandomForestCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    cb_user_item_recommender, interactions_df, items_df))]\n",
    "\n",
    "random_forest_cbui_tts_results = pd.DataFrame(\n",
    "    random_forest_cbui_tts_results,\n",
    "    columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(random_forest_cbui_tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-bosnia",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "through-hands",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoostCBUIRecommender</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.10353</td>\n",
       "      <td>0.160557</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.080747</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>0.132415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Recommender      HR@1     HR@3      HR@5     HR@10    NDCG@1  \\\n",
       "0  XGBoostCBUIRecommender  0.052274  0.10353  0.160557  0.248812  0.052274   \n",
       "\n",
       "     NDCG@3  NDCG@5   NDCG@10  \n",
       "0  0.080747  0.1037  0.132415  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb_user_item_recommender = XGBoostCBUIRecommender(\n",
    "    **{'learning_rate': 0.012153727291261835, 'max_depth': 6.0, 'min_samples_split': 10.0, 'n_estimators': 600.0, 'n_neg_per_pos': 10.0})  \n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "xgboost_cbui_tts_results = [['XGBoostCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    cb_user_item_recommender, interactions_df, items_df))]\n",
    "\n",
    "xgboost_cbui_tts_results = pd.DataFrame(\n",
    "    xgboost_cbui_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(xgboost_cbui_tts_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "right-height",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(cb_user_item_recommender.model.feature_importances_,\n",
    "                             index=cb_user_item_recommender.model_features).sort_values().nlargest(15)\n",
    "feat_importances.sort_values().plot(kind='barh', figsize=[9, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-winner",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-workstation",
   "metadata": {},
   "source": [
    "I used CatBoost due to its superiority of handling categorical features to other GBDT models. It turns out to be a good idea, CatBoost outperforms scikit XGBoost and other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "thick-shopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostCBUIRecommender</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.10387</td>\n",
       "      <td>0.164291</td>\n",
       "      <td>0.252546</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.080783</td>\n",
       "      <td>0.105139</td>\n",
       "      <td>0.133512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Recommender      HR@1     HR@3      HR@5     HR@10    NDCG@1  \\\n",
       "0  CatBoostCBUIRecommender  0.052274  0.10387  0.164291  0.252546  0.052274   \n",
       "\n",
       "     NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.080783  0.105139  0.133512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "catboost_recommender = CatBoostCBUIRecommender(\n",
    "    **{'learning_rate': 0.07358203024749414, 'max_depth': 7.0, 'n_estimators': 1400, 'n_neg_per_pos': 11.0})  \n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "catboost_results = [['CatBoostCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    catboost_recommender, interactions_df, items_df))]\n",
    "\n",
    "catboost_results = pd.DataFrame(\n",
    "    catboost_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(catboost_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "brutal-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat_importances = pd.Series(catboost_recommender.model.feature_importances_,\n",
    "                             index=catboost_recommender.model_features).sort_values().nlargest(15)\n",
    "feat_importances.sort_values().plot(kind='barh', figsize=[9, 7]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-championship",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-double",
   "metadata": {},
   "source": [
    "I was also trying to treat it as classification problem, with probability of 1 as recommendation score, however doesn't work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "lucky-dealing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegressionCBUIRecommender</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.09131</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.233198</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.068725</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.119016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Recommender      HR@1     HR@3      HR@5     HR@10  \\\n",
       "0  LogisticRegressionCBUIRecommender  0.041073  0.09131  0.145282  0.233198   \n",
       "\n",
       "     NDCG@1    NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.041073  0.068725  0.090407  0.119016  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cb_user_item_recommender = LogisticRegressionCBUIRecommender(\n",
    "    **{'n_neg_per_pos': 5, 'max_iter': 1500}) \n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "logreg_cbui_tts_results = [['LogisticRegressionCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    cb_user_item_recommender, interactions_df, items_df))]\n",
    "\n",
    "logreg_cbui_tts_results = pd.DataFrame(\n",
    "    logreg_cbui_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(logreg_cbui_tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-dakota",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "opposite-wallace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVRCBUIRecommender</td>\n",
       "      <td>0.051935</td>\n",
       "      <td>0.108282</td>\n",
       "      <td>0.16463</td>\n",
       "      <td>0.255601</td>\n",
       "      <td>0.051935</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.10711</td>\n",
       "      <td>0.13677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Recommender      HR@1      HR@3     HR@5     HR@10    NDCG@1  \\\n",
       "0  SVRCBUIRecommender  0.051935  0.108282  0.16463  0.255601  0.051935   \n",
       "\n",
       "     NDCG@3   NDCG@5  NDCG@10  \n",
       "0  0.084509  0.10711  0.13677  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 17min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cb_user_item_recommender = SVRCBUIRecommender(**{'C': 1, 'gamma': 0.001, 'n_neg_per_pos': 6})\n",
    "\n",
    "# Give the name of your recommender in the line below\n",
    "svr_cbui_tts_results = [['SVRCBUIRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    cb_user_item_recommender, interactions_df, items_df))]\n",
    "\n",
    "svr_cbui_tts_results = pd.DataFrame(\n",
    "    svr_cbui_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(svr_cbui_tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-medline",
   "metadata": {},
   "source": [
    "## Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "suited-nomination",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AmazonRecommender</td>\n",
       "      <td>0.03666</td>\n",
       "      <td>0.09776</td>\n",
       "      <td>0.139172</td>\n",
       "      <td>0.208418</td>\n",
       "      <td>0.03666</td>\n",
       "      <td>0.071565</td>\n",
       "      <td>0.088627</td>\n",
       "      <td>0.11102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Recommender     HR@1     HR@3      HR@5     HR@10   NDCG@1    NDCG@3  \\\n",
       "0  AmazonRecommender  0.03666  0.09776  0.139172  0.208418  0.03666  0.071565   \n",
       "\n",
       "     NDCG@5  NDCG@10  \n",
       "0  0.088627  0.11102  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from recommenders.amazon_recommender import AmazonRecommender\n",
    "\n",
    "amazon_recommender = AmazonRecommender()\n",
    "\n",
    "amazon_tts_results = [['AmazonRecommender'] + list(evaluate_train_test_split_implicit(\n",
    "    amazon_recommender, interactions_df, items_df))]\n",
    "\n",
    "amazon_tts_results = pd.DataFrame(\n",
    "    amazon_tts_results, columns=['Recommender', 'HR@1', 'HR@3', 'HR@5', 'HR@10', 'NDCG@1', 'NDCG@3', 'NDCG@5', 'NDCG@10'])\n",
    "\n",
    "display(amazon_tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-dating",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "moderate-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recommender</th>\n",
       "      <th>HR@1</th>\n",
       "      <th>HR@3</th>\n",
       "      <th>HR@5</th>\n",
       "      <th>HR@10</th>\n",
       "      <th>NDCG@1</th>\n",
       "      <th>NDCG@3</th>\n",
       "      <th>NDCG@5</th>\n",
       "      <th>NDCG@10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVRCBUIRecommender</td>\n",
       "      <td>0.051935</td>\n",
       "      <td>0.108282</td>\n",
       "      <td>0.164630</td>\n",
       "      <td>0.255601</td>\n",
       "      <td>0.051935</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.107110</td>\n",
       "      <td>0.136770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoostCBUIRecommender</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.103870</td>\n",
       "      <td>0.164291</td>\n",
       "      <td>0.252546</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.080783</td>\n",
       "      <td>0.105139</td>\n",
       "      <td>0.133512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoostCBUIRecommender</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.103530</td>\n",
       "      <td>0.160557</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.052274</td>\n",
       "      <td>0.080747</td>\n",
       "      <td>0.103700</td>\n",
       "      <td>0.132415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestCBUIRecommender</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.089613</td>\n",
       "      <td>0.153768</td>\n",
       "      <td>0.239308</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.063581</td>\n",
       "      <td>0.089649</td>\n",
       "      <td>0.117323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegressionCBUIRecommender</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.091310</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.233198</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.068725</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.119016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LinearRegressionCBUIRecommender</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.091310</td>\n",
       "      <td>0.145282</td>\n",
       "      <td>0.220638</td>\n",
       "      <td>0.041073</td>\n",
       "      <td>0.068725</td>\n",
       "      <td>0.090407</td>\n",
       "      <td>0.115345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AmazonRecommender</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.097760</td>\n",
       "      <td>0.139172</td>\n",
       "      <td>0.208418</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.071565</td>\n",
       "      <td>0.088627</td>\n",
       "      <td>0.111020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Recommender      HR@1      HR@3      HR@5     HR@10  \\\n",
       "0                 SVRCBUIRecommender  0.051935  0.108282  0.164630  0.255601   \n",
       "1            CatBoostCBUIRecommender  0.052274  0.103870  0.164291  0.252546   \n",
       "2             XGBoostCBUIRecommender  0.052274  0.103530  0.160557  0.248812   \n",
       "3        RandomForestCBUIRecommender  0.032926  0.089613  0.153768  0.239308   \n",
       "4  LogisticRegressionCBUIRecommender  0.041073  0.091310  0.145282  0.233198   \n",
       "5    LinearRegressionCBUIRecommender  0.041073  0.091310  0.145282  0.220638   \n",
       "6                  AmazonRecommender  0.036660  0.097760  0.139172  0.208418   \n",
       "\n",
       "     NDCG@1    NDCG@3    NDCG@5   NDCG@10  \n",
       "0  0.051935  0.084509  0.107110  0.136770  \n",
       "1  0.052274  0.080783  0.105139  0.133512  \n",
       "2  0.052274  0.080747  0.103700  0.132415  \n",
       "3  0.032926  0.063581  0.089649  0.117323  \n",
       "4  0.041073  0.068725  0.090407  0.119016  \n",
       "5  0.041073  0.068725  0.090407  0.115345  \n",
       "6  0.036660  0.071565  0.088627  0.111020  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tts_results = pd.concat([linear_reg_cbui_tts_results, random_forest_cbui_tts_results,\n",
    "                         xgboost_cbui_tts_results, catboost_results, logreg_cbui_tts_results,\n",
    "                         svr_cbui_tts_results, amazon_tts_results]).sort_values(by='HR@10',\n",
    "                                                                                ascending=False).reset_index(drop=True)\n",
    "display(tts_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-violation",
   "metadata": {},
   "source": [
    "To sum up, all of my models outperforms Amazon Recommender. Best model turns out to be SVR (0.256), however I'm not so convinced of it, because of its very long training time and lack of interpretability. I am more in favour of CatBoost, which scored very decent 0.253, which is 0.045 more than AmazonRecommender (0.208)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs-class-env",
   "language": "python",
   "name": "rs-class-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
